---
title: "Troubleshooting HPRCC Jobs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Troubleshooting HPRCC Jobs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This guide shows how to diagnose and fix common issues when running jobs on the COH HPRCC clusters. The **hprcc** package integrates with [autometric](https://wlandau.github.io/autometric/) for detailed resource monitoring, allowing you to:

- Track memory and CPU usage over time
- Identify resource bottlenecks 
- Debug failed jobs
- Optimize resource allocation

# Setting Up Job Monitoring

Enable monitoring in your `_targets.R` file:

```r
# Enable SLURM and autometric logging
options(
  hprcc.slurm_logs = TRUE,     # Write logs to _targets/logs/
  hprcc.slurm_jobs = TRUE,     # Keep job scripts for debugging
  hprcc.slurm_verbose = TRUE   # Show SLURM messages in console
)

# Load packages
library(targets)
library(hprcc)
```

This logs HPRCC settings for the run and creates one log file per job in `_targets/logs/`:

- `crew-{jobid}.out`: SLURM output & resource metrics  
- `hprcc_settings_{jobid}.txt`: Environment configuration

# Common Error Patterns

## Memory Issues

### Symptoms
- Job killed by SLURM's OOM killer
- Error: `slurmstepd: error: Exceeded job memory limit`
- Rapid memory growth in resource plots
- Missing results with no error message

### Diagnosis
Check memory usage pattern in logs:

```r
# Read logs for memory analysis 
logs <- read_target_logs("_targets/logs")

# Plot memory usage
plot_target_logs(logs, metric = "rss")

# Look for peak memory usage
max_mem <- max(logs$rss, na.rm = TRUE)
print(paste("Peak memory:", round(max_mem/1e6, 1), "GB"))
```

### Solutions

1. Switch to a higher memory controller:
```r
# For memory-intensive tasks like scRNA-seq
tar_target(seurat_obj,
          CreateSeuratObject(counts) |> 
            SCTransform() |>
            RunPCA(),
          resources = "large_mem")  # 800GB RAM
```

2. Break data into chunks:
```r
# Process in manageable chunks
tar_target(chunks, split(data, ceiling(seq_len(nrow(data))/1000))),
tar_target(processed, process_chunk(chunks),
          pattern = map(chunks),
          resources = "medium")  # 40GB per chunk
```

## CPU Utilization Issues

### Symptoms
- Jobs taking longer than expected  
- Low CPU % in resource plots
- High system time vs user time

### Diagnosis
Check CPU patterns:

```r
# Plot CPU usage over time
plot_target_logs(logs, metric = "cpu")

# Calculate average CPU utilization
mean_cpu <- mean(logs$cpu, na.rm = TRUE)
print(paste("Mean CPU usage:", round(mean_cpu, 1), "%"))
```

Signs of poor CPU utilization:
- CPU usage < 50% of allocated cores
- Large gaps between CPU spikes
- High system time relative to user time

### Solutions

1. Match CPU allocation to usage:
```r
# CPU-bound task
tar_target(parallel_task,
          mclapply(data, heavy_computation, mc.cores = 4),
          resources = "medium")  # 4 cores

# I/O bound task 
tar_target(io_task, 
          read_big_file(path),
          resources = "tiny")  # 2 cores is plenty
```

2. Use appropriate parallelization:
```r 
# Bad - nested parallelism
tar_target(nested_parallel,
          parallel::mclapply(files, function(f) {
            BiocParallel::bplapply(read_data(f), process)
          }))

# Good - single level of parallelism  
tar_target(files, list.files(pattern = "*.bam")),
tar_target(processed,
          process_file(files),
          pattern = map(files),
          resources = "medium")
```

## Container/Environment Issues

### Symptoms
- Missing R packages
- Library load errors
- Inconsistent behavior between Apollo/Gemini

### Diagnosis
Check environment config:

```r
# Print detailed environment info
log_hprcc_settings()

# Check library paths
.libPaths()
```

Examine `hprcc_settings_{jobid}.txt` for:
- R library paths
- Container bind mounts
- Environment variables

### Solutions

1. Set correct library paths:
```r
options(
  hprcc.r_libs_user = "/path/to/my/R/libs",
  hprcc.r_libs_site = "/shared/R/libs"
)
```

2. Add container bind paths:
```r
options(hprcc.singularity_bind_dirs = 
        "/data,/scratch,/labs,/ref_genome")
```

3. Use consistent container:
```r
options(
  hprcc.singularity_container = 
    "/opt/singularity-images/rbioc/vscode-rbioc_3.19.sif"
)
```

# Resource Usage Examples

Here are typical resource patterns for common bioinformatics tasks:

## Single Cell RNA-seq

```
Memory:
- SCTransform: Rapid growth to ~10X input size
- PCA/UMAP: Moderate, stable usage
- Clustering: Spiky with ~2-3X baseline

CPU:
- SCTransform: High (>80%) sustained
- PCA: Bursts of 100% 
- Clustering: Variable 20-100%
```

## Bulk RNA-seq

```
Memory:
- Alignment: Linear growth, then plateau
- Feature counting: Stable, ~2X BAM size
- DESeq2: ~5X count matrix size

CPU:
- Alignment: Sustained high (>90%)
- Counting: Moderate (40-60%)
- DESeq2: Variable, spiky
```

# Best Practices

## Resource Monitoring
- Always enable logging `options(hprcc.slurm_logs = TRUE)`
- Review past job performance with `explore_logs()`
- Monitor memory growth patterns
- Look for CPU bottlenecks

## Resource Allocation
- Start with smallest sufficient controller
- Use retry controller for unpredictable jobs
- Match CPU count to actual parallelization
- Add 20% buffer to observed memory usage

## Pipeline Design
- Break large jobs into smaller chunks
- Avoid nested parallelization
- Cache intermediate results
- Use appropriate storage formats

## Debugging
1. Enable verbose logging
2. Check both SLURM and R errors
3. Reproduce locally if possible
4. Test with smaller dataset
5. Monitor resource usage

## Container Management
- Use consistent containers
- Bind required directories
- Set correct library paths
- Keep environments minimal

# Resource Sizing Guide

| Task Type | Example | Recommended Controller | Notes |
|-----------|---------|----------------------|--------|
| Data Loading | Reading BAM/CSV | `tiny` | I/O bound, low CPU |
| QC/Processing | FastQC, Trimming | `small` | Moderate CPU/mem |
| Alignment | STAR, BWA | `large` | CPU intensive |
| scRNA-seq | Seurat | `large_mem` | Memory hungry |
| Deep Learning | Training | `gpu_medium` | GPU required |

# Getting Help

- Report issues & ask questions : [GitHub Issues](https://github.com/cohmathonc/hprcc/issues)
- Check logs: `explore_logs()`
- Review documentation: `help(package = "hprcc")`