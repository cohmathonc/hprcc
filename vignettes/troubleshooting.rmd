---
title: "Troubleshooting with SLURM and Autometric Logs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Troubleshooting with SLURM and Autometric Logs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

The **hprcc** package integrates with [autometric](https://wlandau.github.io/autometric/) to provide detailed resource monitoring of your SLURM jobs. This guide shows how to enable logging and analyze job performance using both SLURM and autometric logs.

# Enabling Resource Logging 

Enable SLURM and autometric logging in your `_targets.R`:

```r
# Turn on SLURM logging
options(hprcc.slurm_logs = TRUE)
```

This creates two types of logs in your `_targets/logs/` directory:

* `hprcc_settings_{job_id}.txt`: Job configuration and environment
* `crew-{job_id}.out`: SLURM output and autometric resource data

# Understanding Log Files

## HPRCC Settings Log

The `hprcc_settings_{job_id}.txt` log contains crucial configuration information about your job environment:

1. **Cluster Information**
    * Cluster name (apollo/gemini)
    * Node hostname

2. **Job Configuration**
    * R library paths
    * Singularity container settings
    * SLURM partition and account

3. **Resource Allocation**
    * `SLURM_CPUS_ON_NODE`: Number of CPUs allocated
    * `SLURM_MEM_PER_NODE`: Memory allocation in MB
    * `CUDA_VISIBLE_DEVICES`: GPU allocation (if applicable)

## Resource Usage Logs

The autometric package provides tools to analyze the resource usage data in `crew-{job_id}.out`. Here's how to read and visualize a job's performance:

```r
library(autometric)

# Read the log file (collects data every second by default)
log <- log_read("_targets/logs/crew-123456.out", 
                units_memory = "gigabytes",  # Convert to GB
                units_time = "minutes")      # Show time in minutes

# Get a quick summary
head(log)
```

The log data frame contains key metrics:

* `rss`: Resident Set Size (actual memory used)
* `virtual`: Total virtual memory available
* `cpu`: CPU usage as percentage
* `time`: Timestamp (relative to job start)

# Visualizing Resource Usage

Create memory usage plots to identify problems:

```r
# Plot memory usage over time
log_plot(log, metric = "rss", 
         main = "Memory Usage Over Time",
         ylab = "Memory (GB)")

# Plot CPU usage 
log_plot(log, metric = "cpu",
         main = "CPU Utilization",
         ylab = "CPU %")
```

# Common Issues and Solutions

1. **Memory Spikes**
    * Look for sudden jumps in `rss` values
    * Consider using larger memory controller:
    
    ```r
    tar_target(big_job, heavy_calculation(),
              resources = tar_resources(
                crew = tar_resources_crew(controller = "large_mem")
              ))
    ```

2. **CPU Underutilization** 
    * Low `cpu` values may indicate:
        * I/O bottlenecks
        * Inefficient parallelization
    * Try increasing CPU allocation:
    
    ```r
    create_controller("compute_heavy",
                     slurm_cpus = 8,
                     slurm_mem_gigabytes = 32)
    ```

3. **Resource Usage Patterns**
    * Steady memory growth may indicate leaks
    * CPU spikes can show bottlenecks
    * Very low CPU might mean I/O bound

# Best Practices

1. **Check Both Log Types**
    * Review `hprcc_settings` for correct configuration
    * Verify resource allocations match expectations
    * Use autometric logs to track actual usage

2. **Regular Monitoring**
    * Check memory growth patterns
    * Watch for unexpected spikes
    * Note peak resource usage

3. **Keep Logs Organized**
    * Use meaningful job names
    * Set `hprcc.slurm_jobs = TRUE` to preserve job scripts
    * Review logs promptly after failures

4. **Resource Planning**
    * Use log data to size future jobs
    * Plan for peak memory needs
    * Match CPU allocation to workload

# Example: Analyzing Job Performance

Here's a complete example of analyzing both configuration and resource usage:

```r
# First, check job configuration
settings_file <- "_targets/logs/hprcc_settings_123456.txt"
cat(readLines(settings_file), sep = "\n")

# Then analyze resource usage
# Read the logs
log <- log_read("_targets/logs/crew-123456.out")

# Check overall memory usage
log_plot(log, metric = "rss")

# Look at CPU utilization
log_plot(log, metric = "cpu")

# Get summary statistics
summary(log$rss)    # Memory stats
summary(log$cpu)    # CPU stats

# Find peak memory usage
max(log$rss)

# Check memory usage distribution
hist(log$rss, breaks = 50, 
     main = "Memory Usage Distribution",
     xlab = "Memory (GB)")
```

This helps identify:

* Peak resource usage
* Resource usage patterns
* Whether to adjust controller resources