---
title: "Getting started with hprcc"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started with hprcc}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This vignette provides a quick guide on how to get started with **hprcc**, a tool designed for running jobs
efficiently on the City of Hope High-Performance Research Computing Center (COH HPRCC) clusters.

You'll need some familiarity with the {[targets](https://docs.ropensci.org/targets/)} package to use **hprcc**. If you're new to targets, check 
out the [targets user manual](https://books.ropensci.org/targets/) or the excellent 
[Carpentries workshop](https://carpentries-incubator.github.io/targets-workshop/index.html) for a more hands-on tutorial.

# Installation

If you are using [RStudio for Bioconductor](http://hprcc.coh.org/user-guide/rbioc/), no installation is necessary. **hprcc** 
is installed and ready to go.

You can install it locally too, although its functionality is limited off HPRCC clusters:

```
if (!require("remotes")) install.packages("remotes")
remotes::install_github("cohmathonc/hprcc")
```

# Running Jobs with {targets}

The **hprcc** package helps you run {targets} pipelines efficiently on HPRCC clusters by automatically configuring jobs based on their resource needs. Here's a simple example:

```
library(targets)
library(hprcc)

tar_script({
    list(
        # Run locally in the main session
        tar_target(y1, 1 + 1, deployment = "main"),
        
        # Run on a tiny node (2 cores, 8GB RAM, 1 hour)
        tar_target(y2, expensive_calculation(y1), 
                  resources = tar_resources(crew = tar_resources_crew(controller = "tiny"))),
        
        # Run on a small node (2 cores, 20GB RAM, 6 hours) - this is the default
        tar_target(z, y1 + y2)
    )
})

tar_make()
```
Or a memory-Intensive single cell analysis:

```r
# Create a pipeline for Seurat analysis
tar_script({
  library(hprcc)
  library(Seurat)
  
  list(
    # Load data - small controller is fine
    tar_target(counts, read_h5("data/counts.h5"),
              resources = "small"),
              
    # Heavy processing needs more memory
    tar_target(seurat_obj, 
              CreateSeuratObject(counts) |> 
                SCTransform() |>
                RunPCA() |>
                RunUMAP(),
              resources = "large_mem")
  )
})
```

When you load **hprcc**, it automatically configures {targets} with SLURM-friendly settings:

- Uses the fast "qs" storage format instead of "rds"
- Minimizes network traffic by using worker-based storage/retrieval 
- Sets up node controllers for different job sizes:
  - `tiny`: 2 cores, 8GB RAM, 1 hour max 
  - `small`: 2 cores, 20GB RAM, 6 hours max (default)
  - `medium`: 4 cores, 40GB RAM, 6 hours max
  - `large`: 8 cores, 80GB RAM, 6 hours max 
  - `xlarge`: 20 cores, 200GB RAM, 12 hours max
  - `huge`: 40 cores, 200GB RAM, 2 hours max
  - `large_mem`: 8 cores, 800GB RAM, 6 hours max (for memory-intensive jobs)
  - `retry`: Automatically retries failed jobs with increasing resources

On the Gemini cluster, GPU nodes are also available:

  - `gpu_medium`: 4 cores, 60GB RAM, 2 hours max on A100/V100 GPUs
  - `gpu_large`: 8 cores, 120GB RAM, 4 hours max on A100/V100 GPUs

# Logging and Utilities 

**hprcc** includes some handy utilities for working with the cluster:

```
# Initialize {future} multisession computation based on your SLURM allocation
init_multisession()

# Check your current SLURM resource allocation, or from within a job
slurm_allocation()

# Enable detailed SLURM logging (writes to _targets/logs/)
options(hprcc.slurm_logs = TRUE)

# Shiny app to evaluate cluster resource usage (when hprcc.slurm_logs = TRUE)
explore_logs()
```

# Configuration 

You can customize **hprcc**'s behavior through R options or environment variables. Some useful ones:

```
# Stage SLURM job scripts in _targets/jobs/ instead of $TMPDIR
options(hprcc.slurm_jobs = TRUE)

# Show SLURM messages in the console
options(hprcc.slurm_verbose = TRUE)

# Change the SLURM account used for job submission
options(hprcc.slurm_account = "my_account")
```

Check out `?hprcc::package-options` for the full list of configuration options.

# Using Containers

**hprcc** uses Singularity containers to ensure consistent environments across both
Apollo and Gemini clusters. By default, it uses the [RStudio for Bioconductor](http://hprcc.coh.org/user-guide/rbioc/)
container, but you can configure this through options if needed.